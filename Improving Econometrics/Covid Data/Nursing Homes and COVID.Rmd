---
title: "Nursing Homes and COVID"
author: "Connor Krenzer"
date: "11/31/2020"
output: html_document
---

#### **The Story:**

Old people in nursing homes are dying en masse to the coronavirus, skewing the death rate upward and providing public officials greater justification for prolonged lockdowns. This is a common talking point I have heard over the past 6 months and I think econometrics provides a unique opportunity to test it out.

**Why This Story?**

There are two reasons I am researching the coronavirus:

1.  The patients are running the asylum these days, so I have no idea who---or what---to believe. One statistic I heard from a particular political pundit is that the average age of death with COVID is older than life expectancy in the US, which suggests the virus isn't a substantial problem for healthy twenty-somethings who do not have pre-existing conditions. But is that true? What is false? Did Governor Cuomo kill Granny? I have to find that out myself.

2.  This topic provides me with an excellent introductory project on web scraping. The coronavirus is a highly publicized issue, meaning there have been countless articles and reports published on the matter. The data is all there in the articles, but it is not organized in the .csv format I have grown all too accustomed to. Most data collected for this project was pulled directly off webpages or read in from .PDF files that were created from images downloaded from webpages, all with the use of R. I have the opportunity to learn some new R packages and get hands-on experience tidying data from its raw, messy form.

**My Thoughts:**

Politicians rank highly among the handful of people who have benefitted from the coronavirus. I think they love the extra attention a little too much. Did you know who Dan McCoy was at the beginning of March this year [2020]? Me neither. I buy into the story that the nursing home fiasco we've heard so much about caused the number of deaths from the coronavirus to explode. Therefore, I expect variation in the COVID death rate (deaths from COVID) to be explained reasonably well by the rate of nursing home patients in the adult population.

```{r setup, include = FALSE}

# Code chunks will be omitted by defualt.
# Graphs will be included.
knitr::opts_chunk$set(echo = FALSE,
                      include = FALSE,
                      results = "show",
                      error = FALSE,
                      warning = FALSE,
                      message = FALSE)

```

```{r packages-import}

if(!require(pacman)) install.packages("pacman")


# General Packages
pacman::p_load(dplyr, readr,
               ggplot2, patchwork)

# Output Packages
pacman::p_load(knitr, modelsummary)


# Project-specific packages
pacman::p_load(corrplot, car, skedastic,
               pastecs, QuantPsyc, olsrr,
               perturb)

library(backports)


# The csv file
covidData <- read_csv("COVID data.csv",
                    col_types = cols(state = col_character(),
                                     "% Wearing Mask" = col_double(),
                                     "Population Density (p/mi^2)" = col_double(),
                                     "Total Tests per 100k" = col_double(),
                                     "Has Strict Lockdown" = col_double(),
                                     "Elderly" = col_double(),
                                     "Nursing" =  col_double(),
                                     "Death Rate" = col_double(),
                                     "Urban.Density" = col_double()))


```

**The Model:**

Using regression analysis subject to the BLUE method of Least Squares, I will test cross-sectional data from the 50 states and DC using the following equation:

\$Deaths = β~1~ + β~2~Nursing + β~3~Tests + β~4~Elderly + β~5~Mask + β~6~Density + β~7~Lockdown\$

+--------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Variable**             | **Definition**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | **Source**                                                                                                                                                                              |
+:========================:+:=================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================:+:=======================================================================================================================================================================================:+
| Deaths (Y)               | The sum of the increase in deaths from the previous day from October 19th, 2020 to October 23rd, 2020 as a rate per 100000 persons, where deaths are defined as the "Total fatalities with confirmed OR probable case diagnoses (per the expanded CSTE case definition of April 5th, 2020 approved by the CDC) In states where the information is available, it only tracks fatalities with confirmed OR probable COVID-19 case diagnosis where on the death certificate, COVID-19 is listed as an underlying cause of death according to WHO guidelines."                                                                        | The COVID Tracking Project for the death data.                                                                                                                                          |
|                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                         |
|                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Kaiser Family Foundation analysis of Certification and Survey Provider Enhanced Reports (CASPER) data for the population data.                                                          |
+--------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Nursing (β~2~)           | The number of people in nursing homes per 100000 adults.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Kaiser Family Foundation analysis of Certification and Survey Provider Enhanced Reports (CASPER) data; The CASPER system includes data for all certified nursing facilities in the U.S. |
+--------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Tests (β~3~)             | The total number of tests completed on October 23rd per 100000 residents.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Pulled from predictcovid.com, who pulled their data from The COVID Tracking Project (who, of course, compiles their information from many local, regional, state, and federal sources). |
+--------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Elderly (β~4~)           | The percent of the population 65 years and older.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Kaiser Family Foundation analysis of Certification and Survey Provider Enhanced Reports (CASPER) data.                                                                                  |
+--------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Mask (β~5~)              | The percentage of state residents who say they wear a mask in public all or most of the time.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | The Washington Post (who got it from Delphi CovidCast, Carnegie Mellon University).                                                                                                     |
+--------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Density (β~6~)           | The state's 2020 population divided by the land area (miles squared).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Pulled from World Population Review's website, who pulled the data from the US Census State Population Estimates Program.                                                               |
+--------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Lockdown (β~7~)          | Dummy Variable determining the severity of government lockdowns as determined by the Washington Post as of September 11th, 2020 on a scale of "No Restrictions", "Some Minor Restrictions", "Moderate Restrictions", and "Major Restrictions" with zeros being No/Minor Restrictions and ones being Moderate/Major Restrictions. Note: Any state with a "\_CONDITION\_ restrictions vary by region" caveat are pooled in with the severity of the lockdown representative of the entire state (Ex. Illinois says "Moderate restrictions vary by region" but is still considered "Moderate" and therefore is a one in this model). | Washington Post.                                                                                                                                                                        |
+--------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Urban (introduced later) | 2010 urban land area (the "physical city"), measured in square miles, divided by urban population.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Data pulled from newsgeography.com; they collected the data from the U.S. Census Bureau.                                                                                                |
+--------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

+-----------------+---------------+-----------------------------------+---------------------------------+
| Variable        | Expected Sign | Expected Statistical Significance | Expected Practical Significance |
+:===============:+:=============:+:=================================:+:===============================:+
| Nursing (β~2~)  | \+            | Low Significance                  | Moderate Significance           |
+-----------------+---------------+-----------------------------------+---------------------------------+
| Tests (β~3~)    | \-            | Moderate Significance             | Moderate Significance           |
+-----------------+---------------+-----------------------------------+---------------------------------+
| Elderly (β~4~)  | \+            | Moderate Significance             | Moderate Significance           |
+-----------------+---------------+-----------------------------------+---------------------------------+
| Mask (β~5~)     | \-            | High Significance                 | High Significance               |
+-----------------+---------------+-----------------------------------+---------------------------------+
| Density (β~6~)  | \+            | Low Significance                  | Low Significance                |
+-----------------+---------------+-----------------------------------+---------------------------------+
| Lockdown (β~7~) | \-            | Moderate Significance             | Moderate Significance           |
+-----------------+---------------+-----------------------------------+---------------------------------+
| Urban           | \+            | Moderate Significance             | Moderate Significance           |
+-----------------+---------------+-----------------------------------+---------------------------------+

**Hesitations**

A weakness key to my variables deals with time---the timeframes on several of my variables do not line up properly. The most notable of these variables is the lockdown variable, which was last updated on September 11^th^, 2020, while several other variables use data from October 23^rd^, 2020. The urban variable used in later iterations of the model contains data from 2010; though it is unlikely drastic urban expansions have happened from one state to another in the last ten years, more recent data would only increase the reliability of the results. Smaller problems dealing with time are with regards to the Nursing (B2) and Elderly (B4) variables. The data in those are from 2019 and---although the difference will probably not matter with elderly---if people are worried about patients in nursing facilities catching the coronavirus, it is possible that many who were in nursing facilities a year ago have decided to go elsewhere, reducing the accuracy of the nursing variable. Another problem is that DC will heavily skew the population density results. Regardless of the accuracy of this model, the coronavirus' dangers cannot be dismissed if all the variables explain little of the variation in the COVID death rate. Other factors, such as lingering symptoms or permanent health problems from the virus also need consideration in discussions of policy.

```{r descriptive statistics}

death <- stat.desc(covidData$`Death Rate`)[c(4:6, 8, 9, 13)]
mask <- stat.desc(covidData$`% Wearing Mask`)[c(4:6, 8, 9, 13)]
tests <- stat.desc(covidData$`Total Tests per 100k`)[c(4:6, 8, 9, 13)]
elderly <- stat.desc(covidData$Elderly)[c(4:6, 8, 9, 13)]
nursing <- stat.desc(covidData$Nursing)[c(4:6, 8, 9, 13)]
density <- stat.desc(covidData$`Population Density (p/mi^2)`)[c(4:6, 8, 9, 13)]

# Prints descriptive statistics
kable(t(data.frame(death, mask, tests, elderly, nursing, density)), caption = "Descriptive Statistics")

rm(death, mask, tests, elderly, nursing, density)

```

**Simple Regression Models**

The beginning of the analysis applied each explanatory variable in a simple linear regression against the dependent variable, "Death" with a 95 percent confidence interval. The calculations are provided in the table below:

```{r simple linear models}

simple_mask <- lm(data = covidData, formula = `Death Rate` ~ `% Wearing Mask`)

simple_pop <- lm(data = covidData, formula = `Death Rate` ~ `Population Density (p/mi^2)`)

simple_test <- lm(data = covidData, formula = `Death Rate` ~ `Total Tests per 100k`)

simple_lockdown <- lm(data = covidData, formula = `Death Rate` ~ `Has Strict Lockdown`)

simple_elderly <- lm(data = covidData, formula = `Death Rate` ~ `Elderly`)

simple_nursing <- lm(data = covidData, formula = `Death Rate` ~ `Nursing`)


modelsummary(models = list(
            "Mask" = simple_mask,
            "Density" = simple_pop,
            "Test" = simple_test,
            "Lockdown" = simple_lockdown,
            "Elderly" = simple_elderly,
            "Nursing" = simple_nursing),
            statistic = c("p.value", "conf.int", "std.error", "statistic"),
            output = "markdown")

# Removing old variables
rm(simple_mask, simple_pop, simple_test, simple_lockdown, simple_elderly, simple_nursing)

```

The table above shows that two of the explanatory variables have statistically significant p-values at the 0.001 level of significance, and three of the six explanatory variables have statistically significant p-values at the 0.05 level. The explanatory variable this study is most interested in, the number of people in certified nursing facilities per 100,000 adults (Nursing), has an R-square just below 0.29, meaning roughly 29 percent of the variation in the number of new deaths to the coronavirus---controlled for population size---is explained by changes in the number of nursing home residents. The coefficient for the nursing home population, 3.6829, suggests that an increase of one person per 100,000 state residents in nursing facilities will increase the number of people who die from the coronavirus by 3-4 per 100,000 state residents. Among the three variables that are statistically significant, only the lockdown and nursing home variables are practically significant with a beta value greater than one. A one percent increase in the percent of people wearing masks will only decrease the number of deaths due to the coronavirus by roughly one in one million.

Further, it is important to note that all variables were assumed to have a straight-line relationship with the dependent variable. The figure below plots all explanatory variables against the dependent variable. No obvious relationship emerges from any of the variables, so a straight-line relationship will be assumed for each variable. In fact, preliminary tests performed showed regression statistics are slightly improved by adopting a quadratic relationship between the variables, yet no clear, logical explanation why this may be the case is apparent from the data, and no obvious story can be told using a different linear relationship.

```{r preliminary tests, eval = FALSE}

# This chunk will not evaluate when knitting.
# The 'slightly better' non-straight line results mentioned in the text above.

summary(lm(data = covidData, formula = `Death Rate` ~ poly(`% Wearing Mask`, 2)))

summary(lm(data = covidData, formula = `Death Rate` ~ poly(`Population Density (p/mi^2)`, 2)))

summary(lm(data = covidData, formula = `Death Rate` ~ poly(`Total Tests per 100k`, 2)))

summary(lm(data = covidData, formula = `Death Rate` ~ poly(Elderly, 2)))

summary(lm(data = covidData, formula = `Death Rate` ~ poly(Nursing, 2)))

```

```{r plots}

# This section of code plots all the different variables against one another.

noRestr <- covidData %>%
  filter(`Has Strict Lockdown` == 0)

restr <- covidData %>%
  filter(`Has Strict Lockdown` == 1)

noRestr <- ggplot(data = noRestr) +
  geom_boxplot(mapping = aes(`Death Rate`)) +
  xlab("Death Rate (no strict lockdown)")
restr <- ggplot(data = restr) +
  geom_boxplot(mapping = aes(`Death Rate`)) +
  xlab("Death Rate (strict lockdown)") +
  xlim(c(0, 8))

# plotting everything against the death rate
qplot(covidData$Nursing, covidData$`Death Rate`, xlab = "Nursing", ylab = "Death Rate") +
  qplot(covidData$`Total Tests per 100k`, covidData$`Death Rate`, xlab = "Tests", ylab = "Death Rate") +
  qplot(covidData$Elderly, covidData$`Death Rate`, xlab = "Elderly", ylab = "Death Rate") +
  qplot(covidData$`% Wearing Mask`, covidData$`Death Rate`, xlab = "Mask", ylab = "Death Rate") +
  qplot(covidData$`Population Density (p/mi^2)`, covidData$`Death Rate`, xlab = "Density (Excluding DC)", ylab = "Death Rate", xlim = c(0, 1500)) /
  qplot(covidData$`Population Density (p/mi^2)`, covidData$`Death Rate`, xlab = "Density (Including DC)", ylab = "Death Rate") +
  restr / noRestr + plot_annotation(title = "Explanatory Variables vs. Dependent Variable (New Deaths from COVID)")


# Removing old variables
rm(noRestr, restr)

```

**Multivariate Regression Models**

The following table displays the preliminary results estimating the COVID death rate (the dependent variable) using a linear regression model containing a 95 percent confidence interval with all six explanatory variables included.

```{r initial multiple regression}

results <- lm(`Death Rate` ~ `Nursing` +
                `Elderly` + `% Wearing Mask` +
                `Population Density (p/mi^2)`+
                `Total Tests per 100k` +
                `Has Strict Lockdown`, data = covidData,)

modelsummary(models = list(results),
            statistic = c("p.value", "conf.int", "std.error", "statistic"),
            notes = "Predictors: Percent Wearing Masks, State Population Density, Total COVID Tests Per 100k, Percent of Population over 65, Nursing Home Residents Per 100k Adults",
            output = "markdown")

```

The R-square value of .5028 suggests that the variables included in the regression model explain approximately half of the variation in the dependent variable. The far lower adjusted R-square value indicates that many of the variables contribute little to the model, which is supported by the low R-squares found in the simple linear regression models.

Compared with the simple regression models, the t-stats and p-values of the explanatory variables changed drastically. The nursing and mask variables remain significant, yet the lockdown variable's p-value moved from 0.025 in the simple linear regression all the way to 0.952 in the multiple regression. All other variables that were not statistically significant in the simple linear regressions had similar bumps in their p-values. The population density variable, for example, had a p-value of 0.137 in the simple linear regression and moved all the way to 0.777 in the multiple regression.

The increase in p-values across all variables indicates multicollinearity exists among the explanatory variables. To determine the extent that collinearity is an issue with the variables in the model, data for the explanatory variables will be examined both visually and with correlation coefficients.

**Examining Multicollinearity**

After testing for multicollinearity, there is a high likelihood that multiple variables will move in tandem with one another in a linear regression model. The correlation matrix below visualizes the extent to which multicollinearity affects each variable. For a quantitative approach, the table beneath the correlation matrix provides the absolute value of the zero-order correlation coefficients between variables.

```{r correlation}

# The correlation coefficients
correlation <- as.data.frame(cor(covidData[,c(-1 ,-5, -8, -9)]))

# Renaming the variables the way they are in my official model in the document
colnames(correlation) <- c("Mask", "Density", "Tests", "Elderly", "Nursing")
rownames(correlation) <- c("Mask", "Density", "Tests", "Elderly", "Nursing")


# Visualizing correlation between the variables (corrplot takes a matrix as input, I think)
corrplot(corr = as.matrix(correlation),
         title = "Correlation Matrix",
         method = "pie",
         type = "upper",
         tl.srt = 0,
         outline = "black",
         tl.pos = "d",
         tl.col = "black",
         tl.cex = 1.5,
         cl.cex = 1.5,
         diag = T)

# The numeric output
datasummary_correlation(correlation,
                        title = "Zero-Order Correlation Coefficients",
                        output = "markdown")

# Removing old variables
rm(correlation)

```

Of the 10 possible combinations of the explanatory variables---excluding the lockdown variable because Pearson correlations require continuous variables---all but three exhibit a negligible degree of multicollinearity (defined as a zero-order correlation coefficient less than \|0.3\|). To examine the three combinations with a higher degree of multicollinearity, the t-stat between each relationship will be considered. The following table details the statistical significance of the variable combinations with a zero-order correlation greater than \|0.3\|:

```{r testing multicollinearity via regressions}

# linear model of masks against density
pop_mask <- lm(data = covidData, formula = `Population Density (p/mi^2)` ~ `% Wearing Mask`)


# linear model of masks against nursing
nurse_mask <- lm(data = covidData, formula = Nursing ~ `% Wearing Mask`)

# linear model of density against tests
test_density <- lm(data = covidData, formula = `Total Tests per 100k` ~ `Population Density (p/mi^2)`)


modelsummary(models = list("Population vs Masks" = pop_mask,
                           "Nursing vs Masks" = nurse_mask,
                           "Tests vs Masks" = test_density),
            statistic = c("p.value", "conf.int", "std.error", "statistic"),
            output = "markdown")

# Removing old variables
rm(pop_mask, nurse_mask, test_density)

```

As the table shows, all three variables having a substantial zero-order correlation coefficient also have a statistically significant p-values, meaning measures should be taken to reduce multicollinearity's effects on the model.

To explore the problem multicollinearity poses to the linear model, collinearity diagnostics and statistics, including condition indices and the variance inflation factor, were considered. The table below displays the figures calculated from these tests, using the death rate as the dependent variable:

```{r variance proportion}

# The variance proportion table (also provides eigenvalues in column 1)
kable(olsrr::ols_eigen_cindex(results)[, -c(1, 3, 9)],
      digits = 3,
      caption = "Collinearity Diagnostics: Variance Proporitons (Dependent Variable: Death)")

kable(data.frame("VIF" = car::vif(results)), caption = "VIF Score")

```

The table above indicates that there are three conditions with a variance proportion greater than the "acceptable" range of 0.5. When observing the variance of inflation factors, all of the variables are close to 1, meaning there is little correlation among the kth predictor and the remaining predictor variables, and hence the variance of B­~k~ is inflated minimally by the presence of other variables.

Despite the promising results of the collinearity statistics, steps to minimize multicollinearity in the regression model will be pursued, first by removing a variable deemed nonessential to the investigation. If removing variables does not remove multicollinearity enough to make a meaningful difference, a proxy variable will be introduced to attempt to resolve this issue. Finally, if neither of those fix multicollinearity, transformations of the variables in the initial model will be attempted using log-lin, lin-log, and log-log methods to examine the relationship between changes in explanatory variables to describe changes in the dependent variable.

**Resolving Multicollinearity**

*Method 1: Removing nonessential variables*

The best candidate to remove from the model is the population density variable. It has an extreme outlier due to the District of Columbia having a far higher population density than that of entire states and has the two strongest correlations among variables in the model. The District of Columbia was not removed from the model because its observation in the population density variable was the only variable in the entire model containing an outlier more than three sample standard deviations away from the mean. Even after removing the District of Columbia from the dataset, changes in the significance were minimal. Regardless, removing the population density variable will also remove the only outlier in the dataset. Further, population density does not serve as an ideal proxy for how frequently people come into contact with one another (and hence increase the spread of the coronavirus); it is deeply flawed, as people are not dispersed evenly within the state's borders---many (if not most) live in cities while the rest of the state remains rural. The following table shows the results from removing population density variable from the initial regression model:

```{r after removing density}

### After removing the population density variable
results2 <- lm(data = covidData, formula = `Death Rate` ~ Nursing +
                 Elderly +
                 `% Wearing Mask` +
                 `Total Tests per 100k` +
                 `Has Strict Lockdown`)

# The model summary
modelsummary(models = list("After Removing Density" = results2),
            statistic = c("p.value", "conf.int", "std.error", "statistic"),
            output = "markdown")

# The variance proportion table (also provides eigenvalues in column 1)
kable(olsrr::ols_eigen_cindex(results2)[, -c(1, 3, 9)],
      digits = 3,
      caption = "Collinearity Diagnostics: Variance Proporitons (Dependent Variable: Death)")

# The VIF score
kable(data.frame("VIF" = car::vif(results2)), caption = "VIF Score")

# Correlation matrix
correlation2 <- as.data.frame(cor(covidData[,c(-1 ,-5, -8, -9)]))

# Renaming the variables the way they are in my official model in the document
colnames(correlation2) <- c("Mask", "Density", "Tests", "Elderly", "Nursing")
rownames(correlation2) <- c("Mask", "Density", "Tests", "Elderly", "Nursing")

# The numeric output
datasummary_correlation(correlation2,
                        title = "Zero-Order Correlation Coefficients",
                        output = "markdown")

# Removing old variables
rm(correlation2, results2)

```

Although removing the density variable had negligible effects on R-square, the p-values lowered for all values except Tests, which was expected. Since collinearity between masks and nursing is still above the \|0.3\| benchmark, further steps should be taken to soften this issue.

*Method 2: Substituting in proxies*

As noted in the beginning of this section, one of the greatest problems with the state population density variable is that it does not take into account where in a state most people are clustered. In an attempt to remedy the lingering multicollinearity, a proxy variable containing urban population density was substituted in the place of the density by state population and landmass variable. States with more urban land and more people living in it have greater urban densities than states with only a few large cities but extensive landmass. The new regression model is shown in the table below:

\#\#\#\#\#\#\#YOU ARE ON RESULTS5 OF THE CORRELATION SCRIPT!!! ONLY CORRELATION, HETERO, AND AUTOCORRELATION REMAIN TO FIX!!!

```{r proxy substitution}

### After adding the urban density variable
results3 <- lm(data = covidData, formula = `Death Rate` ~ Nursing +
                 Elderly +
                 `% Wearing Mask` +
                 `Total Tests per 100k` +
                 `Has Strict Lockdown` + 
                 `Urban.Density`)

# The model summary
modelsummary(models = list("Multivariate Regression Model Statistics, with Urban Population Density" = results3),
            statistic = c("p.value", "conf.int", "std.error", "statistic"),
            output = "markdown")


# Correlation matrix
correlation3 <- as.data.frame(cor(covidData[,c(-1 ,-5, -8, -9)]))

# Renaming the variables the way they are in my official model in the document
colnames(correlation3) <- c("Mask", "Density", "Tests", "Elderly", "Nursing")
rownames(correlation3) <- c("Mask", "Density", "Tests", "Elderly", "Nursing")

# The numeric output
datasummary_correlation(correlation3,
                        title = "Zero-Order Correlation Coefficients",
                        output = "markdown")


# The variance proportion table (also provides eigenvalues in column 1)
kable(olsrr::ols_eigen_cindex(results3)[, -c(1, 3, 9)],
      digits = 3,
      caption = "Collinearity Diagnostics: Variance Proporitons (Dependent Variable: Death)")

# The VIF score
kable(data.frame("VIF" = car::vif(results3)), caption = "VIF Score")


# Removing old variables
rm(correlation3, results3)

```

As the charts above illustrate, substituting urban density in for state population density did not resolve multicollinearity in the regression model. Therefore, the use of urban population density is not justified.

*Method 3: Changing the form of the initial regression equation*

[Log-Lin Method:]{.ul} The "log-lin" method of correcting multicollinearity takes the natural logarithm of the explanatory variables and repeats the regression using the new, transformed explanatory variables. The dependent variable remains unchanged. It did not improve the p-values of the variables compared to the initial regression.

```{r log lin, eval = FALSE}

# This chunk does not evaluate--it is meant to prove that I am not
# a dirty liar in the writing above.

# Variable transformations (lockdown stays the same)
# "Log-Lin" method
summary(lm(data = covidData, formula = `Death Rate` ~ I(log(Nursing)) +
                 I(log(Elderly)) +
                 I(log(`% Wearing Mask`)) +
                 I(log(`Total Tests per 100k`)) +
                 `Has Strict Lockdown` + 
                 I(log(`Urban.Density`))))

# The results did not improve

```

[Lin-Log:]{.ul} The "lin-log" method of correcting multicollinearity takes the natural log of the dependent variable and performs the regression using the original explanatory variables on the transformed dependent variable. There is a problem with this method in particular because Death (Y) contains values of zero, which is outside the domain of the natural logarithm. For the period sampled for this study, neither Maine nor Vermont had any new deaths due to the coronavirus. To allow this transformation to take place, the zeroes were replaced with values arbitrarily close to zero. The results of the new regression are provided in the tables below:

```{r lin log}

# "Lin-Log" method
deathLN <- log(covidData$`Death Rate`)

# Death has values that are zero, so we must replace them with a very small value
deathLN[22] <- 0.00000000000000000000000000000000001
deathLN[47] <- 0.00000000000000000000000000000000001

linLog <- lm(data = covidData, formula = deathLN ~ Nursing +
                 Elderly +
                 `% Wearing Mask` +
                 `Total Tests per 100k` +
                 `Has Strict Lockdown` + 
                 `Population Density (p/mi^2)`)

# The model summary
modelsummary(models = list("Multivariate Regression Model Statistics After Lin-Log Transformation" = linLog),
            statistic = c("p.value", "conf.int", "std.error", "statistic"),
            output = "markdown")

# Removing old variables
rm(linLog)

```

This transformation improved the amount of variation the predictors explain in the natural log transform of the dependent variable. From the initial regression, the difference between the R-square and adjusted R-square remained roughly the same in this new model, however, the R-square in the new model is considerably greater at .568. The significance of the variables remains unchanged--only masks and the nursing home coefficients are significant.

[Log-Log:]{.ul} The "log-log" method of correcting multicollinearity involves taking the natural logarithm of both the dependent variable and the explanatory variables. As with the "lin-log" method, the values for Maine and Vermont were given arbitrarily small values to allow the natural logarithm to remain within its domain. This method did not improve the statistical significance of the variables in the regression compared to the original regression, although similar effects on the R-square were found between this and the 'lin log' model.

```{r log log, eval = FALSE}

# "Log-Log" method
summary(lm(data = covidData, formula = deathLN ~ I(log(Nursing)) +
                 I(log(Elderly)) +
                 I(log(`% Wearing Mask`)) +
                 I(log(`Total Tests per 100k`)) +
                 `Has Strict Lockdown` + 
                 I(log(`Urban.Density`))))

```

*An Improved Model*

The degree of multicollinearity within the original model mandated that measures be taken to alter the equation. Removing density from the model made modest improvements in the significance of variables, but the gains were merely incremental and changed little in the regression results. Substituting in a proxy variable for the state population density did not improve the results on the variables in their original form. Variable transformation proved very effective at improving the results of the regressions. Although not shown in the regressions above, the p-value in a simple linear regression of new COVID deaths against urban density was significant. This made me curious to see whether a "lin-log" transformation of the urban variable would improve the results of the regression. Intuitively, the urban density variable is a better proxy for how frequently people come into contact with one another than the state population density and should therefore make a more accurate predictor in the model. Despite the fact that multicollinearity in the model increased after replacing state population density with urban density, the "lin-log" method should have eased the problems posed by the greater level of multicollinearity. The results of the regression using urban density and the "lin-log" method are the following:

```{r simple log density, eval = FALSE}

# The simple linear regression model described above.
summary(lm(deathLN ~ covidData$Urban.Density))

```

```{r improved model}

results4 <- lm(data = covidData, formula = deathLN ~ Nursing +
                 Elderly +
                 `% Wearing Mask` +
                 `Total Tests per 100k` +
                 `Has Strict Lockdown` + 
                 `Urban.Density`)


# The model summary
modelsummary(models = list("Multivariate Regression Model Statistics After Lin-Log Transformation, With Urban Density" = results4),
            statistic = c("p.value", "conf.int", "std.error", "statistic"),
            output = "markdown")

# Removing old variables
rm(results4)

```

This did not improve the model, more than the model presented in the 'lin-log' section. Removing the elderly variable from the regression is an excellent choice to increase the degrees of freedom while also reducing the level of multicollinearity among the variables, however. Keeping both a rate of the number of people in nursing facilities---people who are typically very old---and the percent of the population over age 65 seems redundant. The reason the elderly population variable was included was to have a proxy for the population of people who are most susceptible to the virus. The nursing facility variable already fills this role, so there is little justification for keeping the Elderly variable in the model. The results of the 'lin-log' model after this removal are as follows:

```{r final model}

results5 <- lm(data = covidData, formula = deathLN ~ Nursing +
                 `% Wearing Mask` +
                 `Total Tests per 100k` +
                 `Has Strict Lockdown` + 
                 `Urban.Density`)

# The model summary
modelsummary(models = list("Multivariate Regression Model Statistics After Lin-Log Transformation, Without Elderly" = results5),
            statistic = c("p.value", "conf.int", "std.error", "statistic"),
            output = "markdown")

# Removing old variables
rm(results, linLog)

```

This revised model has reduced multicollinearity to a more acceptable level and improved the significance of the predictors in the process. The multicollinearity which remains is likely minimal and should not dramatically affect the conclusions drawn from this analysis. The measures taken to reduce multicollinearity also lowered the p-values dramatically and doubled the number of significant variables from two to four. Although the R-square dropped by .05 in this revised model, it is still preferable to the initial model because the results obtained in the revised model are considerably more reliable with respect to the significance of the variables.

*Identifying Heteroscedasticity*

[Step 1. Visual Analysis of Data]{.ul}: Homoscedasticity, perhaps known better as "equal spread", of the data in the regression is necessary for the assumptions of OLS to hold. If the standard error at a given area of the sample regression function (SRF) is not the same as all the other areas of the SRF, the model's results may be misleading. After visual analysis, the data appears to have some semblance of homoscedasticity.

```{r Death vs standardized predicted vals}

# This section checks for heteroscedasticity in the data
ggplot(mapping = aes(y = deathLN,x = Normalize(residuals.lm(results5)))) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  xlab("Regression Standardized Predicted Values") +
  ylab("Death") +
  ggtitle("Dependent Variable: Death\n") +
  theme(plot.title = element_text(hjust = 0.5))

```

In addition to the visual analysis of the regression, the form of the predicted values of both the independent (Yi-hat) and dependent variables (Xi-hat) when graphed against residual squared (ui-hat^2^) values should be examined. A "parallel band" in the data is the only desired pattern in the output, as it suggests homoscedasticity. Horizontal lines have been added to the plots for ease of reading:

```{r hetero}

# The error terms
error <- residuals.lm(results5)
# The square of the error terms
errorsq <- error^2

# Plotting the error terms against the
# dependent variable
heteroDep <- ggplot() +
  geom_point(mapping = aes(x = deathLN, y = errorsq)) +
  geom_hline(yintercept = 0.225) +
  geom_hline(yintercept = -0.01) +
  xlab("Death") +
  ylab("Residual Squared") +
  ggtitle("Examining for Heteroscedasticity") +
  theme(plot.title = element_text(hjust = 0.5))


# Plotting the error terms against the
# predicted values
heteroIndep <- ggplot() +
  geom_point(mapping = aes(x = predict.lm(results5), y = errorsq)) +
  geom_hline(yintercept = 0.225) +
  geom_hline(yintercept = -0.01) +
  xlab("Predicted Value (Xi)") +
  ylab("Residual Squared")


heteroDep / heteroIndep

# Removing old variables
rm(heteroDep, heteroIndep, results4)

```

When graphed against the residual squared values, Yi and Xi exhibit moderate levels of heteroscedasticity. Empirical investigation into the presence of heteroscedasticity of the data is necessary to draw more decisive conclusions.

[Step 3. Empirical Analysis: Glejser Method]{.ul}

The Glejser method for determining heteroscedasticity provides various means for evaluating the degree of heteroscedasticity within the data by running regression models on the error terms. The Glejser tests used in this analysis take the following form:

$|u-hati| = B1 + B2Xi + . . . + BkXk$

or

$|u-hati2| = B1 + B2Xi + . . . + BkXk$

The null hypothesis states that the data is homoscedastic, so if the regression statistics have a high p-value and a low R-square, the data has homoscedastic data. If the Glejser test's null hypothesis is rejected, the model has heteroscedastic data. The results of the first Glejser test for the data used in this model are as follows:

```{r glejser abs error}

# Stores the absolute value of the error term
absError <- abs(error)

glej1 <- lm(data = covidData, formula = absError ~ Nursing +
                 `% Wearing Mask` +
                 `Total Tests per 100k` +
                 `Has Strict Lockdown` + 
                 `Urban.Density`)
# The model summary
modelsummary(models = list("Glejser Test (Absolute Value of Residuals is Dependent Variable)" = glej1),
            statistic = c("p.value", "conf.int", "std.error", "statistic"),
            output = "markdown")

# Removing old variables
rm(glej1)

```

The results from the second Glejser test are as follows:

```{r glejser errorsq}

glej2 <- lm(data = covidData, formula = absError ~ Nursing +
                 `% Wearing Mask` +
                 `Total Tests per 100k` +
                 `Has Strict Lockdown` + 
                 `Urban.Density`)
# The model summary
modelsummary(models = list("Glejser Test (Squared Residuals is Dependent Variable)" = glej2),
            statistic = c("p.value", "conf.int", "std.error", "statistic"),
            output = "markdown")

# Removing old variables
rm(glej2)

```

Fortunately, none of the predictors are significant, meaning heteroskedasticity is not a 'big enough' problem to invalidate this study's conclusions.

To simplify the analysis, there are two versions of the regression model which will be discussed for the remainder of the paper. The first is the initial regression, which was the very first multiple regression in this paper. The other is the revised model, which involved taking the natural logarithm of the dependent variable--the regression labeled "Multivariate Regression Model Statistics After Lin-Log Transformation, Without Elderly."

*Testing for Autocorrelation*

Autocorrelation refers to the degree of correlation between the values of the same variable across different observations in the data. Autocorrelation occurs most often in time series and panel data, and can be caused by a multitude of factors, though it is quite rare in cross-sectional data. To test for autocorrelation, the Durbin-Watson test will be performed on the data. If the D-W statistic is close to 2.0, the data are unlikely to be autocorrelated. The results from the Durbin-Watson test using the data from the final regression are shown below:

```{r durbin watson}

# This section checks the dataset for autocorrelation

# Testing the model for autocorrelation using the Durbin-Watson test
durbinWatsonTest(results5) %>% 
  unclass() %>% # stores the results in a list
  unlist() %>% # stores the results in a character vector
  t() %>% # transpose to put the values in a column
  kable()

## Since the p-value is greater than 0.05, we cannot reject the null
## hypothesis (which states that the variables have NO autocorrelation).
## We can conclude, therefore, that the variables are NOT autocorrelated.

rm(absError, deathLN, error, errorsq)

```

Since the D-W statistic is close to 2.0, it is reasonable to dismiss the possibility that the data are autocorrelated. This result is not surprising because the data used in this study is cross-sectional.

**Conclusion**

The data used in the regressions provide a unique look into the various factors influencing the number of people who die from the coronavirus. One conclusion which was very interesting was that the elderly variable (β~4~) was not significant in any of the regressions performed in the study. This disputes the narrative claiming states with older populations, such as Florida, are more likely to have higher death rates solely due to the fact that there are more people over the age 65 as a percent of the population. The data in this study provides mixed results on the role lockdowns play on the death rate. It was significant in the simple regression model, though lingering multicollinearity may have raised its p-value. Judging by the results of the final regression, the lockdown variable was not significant enough to rule out the possibility that the true value of β~7~ was zero. This suggests strict lockdowns do not have one of their most important intended effects---reducing the number of deaths due to COVID. Regardless, the significance of the lockdown variable should not be entirely trusted because of the time disparity. To reiterate from the Hesitations section of the paper, this variable should be recalculated with more accurate data; simply because a state had a lockdown in mid-September does not necessarily mean they have a strict lockdown in mid-October---the policy could have since reversed for states with or without a strict lockdown.

The Mask (β~5~) variable was highly significant in all regressions done in this study. In each revision of the model, however, the masks variable had a coefficient close to zero, although it was negative. This suggests that masks only have a modest impact on the increase in deaths. Put differently, masks only reduce the deaths due to COVID modestly. Despite low practical significance, masks do what was predicted at the start of beginning of this paper. The more people wear masks, the fewer the deaths due to COVID---controlled for population.

In all regressions, the variable of interest---Nursing (B2)---was highly significant. What is worth mentioning about this variable is that it says nothing about the conditions of the facilities in which the residents live, staffing shortages, or a whole host of other factors relevant to the spread of the virus. Simply the number of patients per the adult population was enough to make the variable very significant. Future research taking discrepancies between facilities into account may tell a very interesting story. This outcome is consistent with the publicity surrounding deaths coming from nursing homes throughout the country. The nursing home variable supports the notion that the coronavirus has hit these facilities especially hard and claimed the lives of many. In a practical sense, these results support what people have thought all along: those at greatest risk of developing terminal illness---those in nursing facilities---should take the utmost care to avoid close contact with carriers of the virus. I leave for the reader the decision on whether the Governor of New York State has any culpability in Granny's death.
